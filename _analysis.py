# -*- coding: utf-8 -*-
"""_analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kx3yF_R5QRj-JxnIuyZ86JAfGrRxoPSN
"""

import pandas as pd
import numpy as np

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Path to your CSV file in Google Drive
file_path = '/content/drive/My Drive/stocks_data/all_stocks_historical_data.csv'


# for local connect
# file_path = 'all_stocks_historical_data.csv'




# Read the CSV file into a DataFrame
df = pd.read_csv(file_path)

df = df.sort_values(by=['instrument_token', 'timestamps'])

# Display the first few rows of the dataframe
df.head()

df = df.drop(['oi', 'created_at'], axis=1)

def print_nan_counts(df):
    """
    This function checks for NaN values in each column of the given DataFrame
    and prints the column names along with their NaN counts and percentage of total records.

    Parameters:
    df (pandas.DataFrame): The DataFrame to check for NaN values.
    """
    total_records = len(df)

    # Check for NaN values in each column and get their count
    nan_counts = df.isna().sum()

    # Filter columns that have NaN values and their counts
    nan_columns = nan_counts[nan_counts > 0]

    # Calculate the percentage of NaN values in each column
    nan_percentages = (nan_columns / total_records) * 100

    # Print each column name and its NaN count and percentage explicitly
    if nan_columns.empty:
        print("No NaN values found.")
    else:
        for column, count in nan_columns.items():
            percentage = nan_percentages[column]
            print(f'{column}: {count} ({percentage:.2f}%)')

# check nan values in data frame
print_nan_counts(df)

# List of trading symbols to filter
symbols = ['INDIA VIX', 'NIFTY 50', 'NIFTY 500',
       'NIFTY AUTO', 'NIFTY BANK',
       'NIFTY COMMODITIES', 'NIFTY CONSR DURBL', 'NIFTY CONSUMPTION',
       'NIFTY CPSE', 'NIFTY ENERGY',
       'NIFTY FMCG',
       'NIFTY HEALTHCARE', 'NIFTY INDIA MFG',
        'NIFTY IT',
       'NIFTY MEDIA', 'NIFTY METAL', 'NIFTY MICROCAP250',
       'NIFTY PHARMA', 'NIFTY PSE', 'NIFTY PSU BANK', 'NIFTY PVT BANK',
       'NIFTY REALTY',  'NIFTY SMLCAP 100']

# Filter the dataframe for the given symbols
filtered_df = df[df['tradingsymbol'].isin(symbols)]

# Pivot the dataframe to get each trading symbol as a column
pivoted_df = filtered_df.pivot(index='timestamps', columns='tradingsymbol', values='close_price')

def g(pivoted_df):
    pct_change = pivoted_df.pct_change() * 100
    return pct_change.sort_index(axis=0)

pivoted_df = g(pivoted_df.copy())


# Display the first few rows of the pivoted dataframe
print(pivoted_df.head())

# Ensure timestamps is a column in pivoted_df for merging
pivoted_df.reset_index(inplace=True)

# Merge the two dataframes on 'timestamps' column
# Assuming you want to keep all rows and columns from the original df and just add the closing prices from pivoted_df
merged_df = pd.merge(df, pivoted_df, on='timestamps', how='left')

# Display the first few rows of the merged dataframe
print(merged_df.head())

# check nan values in data frame
print_nan_counts(merged_df)

def fill_nan_values(df):
    """
    This function fills NaN values in each column of the given DataFrame
    with the mean of that column. If the column is entirely NaN, it fills with 0.

    Parameters:
    df (pandas.DataFrame): The DataFrame in which NaN values will be filled.
    """
    for column in df.columns:
        # Check if the column has NaN values
        if df[column].isna().sum() > 0:
            mean_value = df[column].mean()
            # If the mean is NaN (column is entirely NaN), set to 0 or another appropriate value
            if pd.isna(mean_value):
                df[column].fillna(0, inplace=True)
            else:
                df[column].fillna(mean_value, inplace=True)

    return df

# check nan values in data frame
print_nan_counts(merged_df)

def print_total_records(df):
    """
    This function prints the total number of records in the given DataFrame.

    Parameters:
    df (pandas.DataFrame): The DataFrame to check the total records of.
    """
    total_records = len(df)
    print(f'Total records: {total_records}')

print_total_records(merged_df)

# Sort the dataframe by 'timestamps' column in ascending order (oldest to newest)
merged_df = merged_df.sort_values(by=['instrument_token', 'timestamps'])

# Display the first few rows of the sorted dataframe
merged_df.head()

merged_df.tail()

# Set the maximum number of columns to display to None (unlimited)
pd.options.display.max_columns = None

print(merged_df.tail())

# Ensure 'timestamps' column is in datetime format
merged_df['timestamps'] = pd.to_datetime(merged_df['timestamps'])

# Sort the DataFrame by 'instrument_token' and 'timestamps' to ensure chronological order for each instrument
merged_df = merged_df.sort_values(by=['instrument_token', 'timestamps'])

# Calculate the percentage change in 'close_price' for each 'instrument_token'
# This will compare each day's close price with the previous day's close price for the same instrument_token
# Using transform to ensure alignment and adding a small number (epsilon) to avoid division by zero
epsilon = 1e-8
merged_df['%changedaily'] = merged_df.groupby('instrument_token')['close_price'].transform(lambda x: (x.diff() / (x.shift() + epsilon)) * 100)

# Display the first few rows to confirm the new column has been added correctly
print(merged_df.head())

# check nan values in data frame
print_nan_counts(merged_df)
# Using the function to fill NaN values in merged_df
merged_df_filled = fill_nan_values(merged_df)
# check nan values in data frame
print_nan_counts(merged_df)

# Define a small number to prevent division by zero
epsilon = 1e-8

# Function to calculate percentage change with epsilon to avoid division by zero
def safe_pct_change(series, periods):
    return (series.diff(periods) / (series.shift(periods) + epsilon)) * 100

# Calculate weekly percentage change
merged_df['%change_weekly'] = merged_df.groupby('instrument_token')['close_price'].transform(lambda x: safe_pct_change(x, periods=5))

# Calculate monthly percentage change
merged_df['%change_monthly'] = merged_df.groupby('instrument_token')['close_price'].transform(lambda x: safe_pct_change(x, periods=21))

# Calculate quarterly percentage change
merged_df['%change_quarterly'] = merged_df.groupby('instrument_token')['close_price'].transform(lambda x: safe_pct_change(x, periods=63))

# Calculate yearly percentage change
merged_df['%change_yearly'] = merged_df.groupby('instrument_token')['close_price'].transform(lambda x: safe_pct_change(x, periods=252))

# Display the last few rows to confirm the new columns
print(merged_df.tail())

# check nan values in data frame
print_nan_counts(merged_df)
# Using the function to fill NaN values in merged_df
merged_df_filled = fill_nan_values(merged_df)
# check nan values in data frame
print_nan_counts(merged_df)

# Ensure 'timestamps' column is in datetime format
merged_df['timestamps'] = pd.to_datetime(merged_df['timestamps'])

# Sort the DataFrame by 'instrument_token' and 'timestamps' to ensure chronological order for each instrument
merged_df = merged_df.sort_values(by=['instrument_token', 'timestamps'])

# Calculate EMA for the specified periods for each 'instrument_token'
ema_periods = [5, 75, 200]
for period in ema_periods:
    column_name = f'EMA_{period}'
    merged_df[column_name] = merged_df.groupby('instrument_token')['close_price'].transform(lambda x: x.ewm(span=period, adjust=True).mean())

# Display the first few rows to confirm the new columns
print(merged_df[['timestamps', 'instrument_token', 'close_price', 'EMA_5', 'EMA_75', 'EMA_200']].tail())

# check nan values in data frame
print_nan_counts(merged_df)
# Using the function to fill NaN values in merged_df
merged_df_filled = fill_nan_values(merged_df)
# check nan values in data frame
print_nan_counts(merged_df)

def calculate_rsi(series, period=14):
    # Calculate daily price changes
    delta = series.diff(1)

    # Separate gains and losses
    gain = delta.clip(lower=0)
    loss = -delta.clip(upper=0)

    # Calculate the average of gains and losses
    avg_gain = gain.rolling(window=period).mean()
    avg_loss = loss.rolling(window=period).mean()

    # Calculate the relative strength (RS)
    RS = avg_gain / avg_loss

    # Calculate the RSI
    RSI = 100 - (100 / (1 + RS))
    return RSI

# Ensure the 'timestamps' column is in datetime format and sort the DataFrame
merged_df['timestamps'] = pd.to_datetime(merged_df['timestamps'])
merged_df = merged_df.sort_values(by=['instrument_token', 'timestamps'])

# Calculate RSI for specified periods and add to the DataFrame
merged_df['RSI_9'] = merged_df.groupby('instrument_token')['close_price'].transform(lambda x: calculate_rsi(x, 9))
merged_df['RSI_14'] = merged_df.groupby('instrument_token')['close_price'].transform(lambda x: calculate_rsi(x, 14))
merged_df['RSI_21'] = merged_df.groupby('instrument_token')['close_price'].transform(lambda x: calculate_rsi(x, 21))

# Display the last few rows to confirm the new columns
print(merged_df[['timestamps', 'instrument_token', 'close_price', 'RSI_9', 'RSI_14', 'RSI_21']].tail())

# check nan values in data frame
print_nan_counts(merged_df)
# Using the function to fill NaN values in merged_df
merged_df_filled = fill_nan_values(merged_df)
# check nan values in data frame
print_nan_counts(merged_df)

print(pd.__version__)

# Function to calculate mean deviation and CCI
def calculate_indicators(df, periods):
    # Pre-calculate Typical Price and store it to avoid recalculating
    TP = (df['high_price'] + df['low_price'] + df['close_price']) / 3
    df['TP'] = TP

    for period in periods:
        # Mean deviation calculation
        SMA_TP = TP.rolling(window=period).mean()
        deviation = TP - SMA_TP
        mean_deviation = deviation.abs().rolling(window=period).mean()
        df[f'mean_deviation_{period}'] = mean_deviation

        # CCI calculation using the previously calculated mean_deviation
        CCI = (TP - SMA_TP) / (0.015 * mean_deviation)
        df[f'CCI_{period}'] = CCI

    # Drop the 'TP' column as it's no longer needed after calculations
    df.drop('TP', axis=1, inplace=True)

    return df

# Assuming 'merged_df' is a pre-existing pandas DataFrame with necessary columns
# Pre-process the DataFrame
merged_df = merged_df.reset_index(drop=True)
merged_df = merged_df.set_index('instrument_token')
merged_df = merged_df.sort_index(level=[0, 1])  # Sort by 'instrument_token' and then timestamps

# Define periods for calculation
periods = [9, 14, 21]

# Calculate mean deviation and CCI in one go to avoid extra memory usage
merged_df = merged_df.groupby('instrument_token', group_keys=False).apply(calculate_indicators, periods)

# Resetting the index after processing
merged_df = merged_df.reset_index(drop=False)

# Display the last few rows to verify the calculations
columns_to_display = ['timestamps', 'instrument_token', 'close_price',
                      'mean_deviation_9', 'mean_deviation_14', 'mean_deviation_21',
                      'CCI_9', 'CCI_14', 'CCI_21']
print(merged_df[columns_to_display].tail())

# check nan values in data frame
print_nan_counts(merged_df)
# Using the function to fill NaN values in merged_df
merged_df_filled = fill_nan_values(merged_df)
# check nan values in data frame
print_nan_counts(merged_df)

# Ensure the 'timestamps' column is in datetime format and sort the DataFrame
merged_df['timestamps'] = pd.to_datetime(merged_df['timestamps'])
merged_df = merged_df.sort_values(by=['instrument_token', 'timestamps'])

# Directly calculate the MACD for each 'instrument_token' by subtracting the 26-period EMA from the 12-period EMA
merged_df['MACD'] = merged_df.groupby('instrument_token')['close_price'].transform(
    lambda x: x.ewm(span=12, adjust=False).mean()) - merged_df.groupby('instrument_token')['close_price'].transform(
    lambda x: x.ewm(span=26, adjust=False).mean())

# Calculate the signal line for each 'instrument_token' as the 9-period EMA of the MACD
merged_df['MACD_signal'] = merged_df.groupby('instrument_token')['MACD'].transform(
    lambda x: x.ewm(span=9, adjust=False).mean())

# Display the last few rows to confirm the new columns
print(merged_df[['timestamps', 'instrument_token', 'close_price', 'MACD', 'MACD_signal']].tail())

# check nan values in data frame
print_nan_counts(merged_df)
# Using the function to fill NaN values in merged_df
merged_df_filled = fill_nan_values(merged_df)
# check nan values in data frame
print_nan_counts(merged_df)

# Ensure the 'timestamps' column is in datetime format and sort the DataFrame
merged_df['timestamps'] = pd.to_datetime(merged_df['timestamps'])
merged_df = merged_df.sort_values(by=['instrument_token', 'timestamps'])

# Directly calculate the percentage change of the current close_price with the VWAP on a rolling 5-day basis
merged_df['%change_close_VWAP'] = merged_df.groupby('instrument_token').apply(
    lambda x: ((x['close_price'] - (x['close_price'] * x['volume']).rolling(window=5).sum() / x['volume'].rolling(window=5).sum())
               / ((x['close_price'] * x['volume']).rolling(window=5).sum() / x['volume'].rolling(window=5).sum())) * 100
).reset_index(level=0, drop=True)

# Display the last few rows to confirm the new column and the calculation
print(merged_df[['timestamps', 'instrument_token', 'close_price', 'volume', '%change_close_VWAP']].tail())

# Assuming merged_df is already defined and includes 'timestamps', 'instrument_token', 'close_price', and 'volume'

# Preprocessing to ensure the data is sorted properly
merged_df = merged_df.sort_values(by=['instrument_token', 'timestamps'])

# Define a function to calculate percentage changes from VWAP without storing VWAP values in the DataFrame
def calculate_percentage_changes(group):
    # Calculate VWAP within the group but don't store in the DataFrame
    VWAP_weekly = (group['close_price'] * group['volume']).rolling(window=5).sum() / group['volume'].rolling(window=5).sum()
    VWAP_monthly = (group['close_price'] * group['volume']).rolling(window=21).sum() / group['volume'].rolling(window=21).sum()
    VWAP_yearly = (group['close_price'] * group['volume']).rolling(window=252).sum() / group['volume'].rolling(window=252).sum()

    # Calculate and store percentage changes directly in the group DataFrame
    group['%change_close_VWAP_monthly'] = ((group['close_price'] - VWAP_monthly) / VWAP_monthly) * 100
    group['%change_close_VWAP_yearly'] = ((group['close_price'] - VWAP_yearly) / VWAP_yearly) * 100

    # Replace potential infinite values with NaN in percentage changes
    group['%change_close_VWAP_monthly'].replace([np.inf, -np.inf], np.nan, inplace=True)
    group['%change_close_VWAP_yearly'].replace([np.inf, -np.inf], np.nan, inplace=True)

    return group

# Apply the function to each group to calculate and add percentage changes
merged_df = merged_df.groupby('instrument_token').apply(calculate_percentage_changes)

# Display the last few rows to confirm the new columns
print(merged_df[['timestamps', 'instrument_token', 'close_price', 'volume', '%change_close_VWAP_monthly', '%change_close_VWAP_yearly']].tail())

# Function to calculate CCI (without storing mean deviation separately)
def calculate_indicators(df, periods):
    # Pre-calculate Typical Price and store it to avoid recalculating
    TP = (df['high_price'] + df['low_price'] + df['close_price']) / 3
    df['TP'] = TP

    for period in periods:
        # Mean deviation calculation only for use in CCI formula
        SMA_TP = TP.rolling(window=period).mean()
        mean_deviation = (TP - SMA_TP).abs().rolling(window=period).mean()

        # CCI calculation using the mean_deviation
        CCI = (TP - SMA_TP) / (0.015 * mean_deviation)
        df[f'CCI_{period}'] = CCI

    # Drop the 'TP' column as it's no longer needed after calculations
    df.drop('TP', axis=1, inplace=True)

    return df

# Assuming 'merged_df' is a pre-existing pandas DataFrame with necessary columns
# Pre-process the DataFrame
merged_df = merged_df.reset_index(drop=True)
merged_df = merged_df.set_index('instrument_token')
merged_df = merged_df.sort_index(level=[0, 1])  # Sort by 'instrument_token' and then timestamps

# Define periods for calculation
periods = [9, 14, 21]

# Calculate CCI in one go to avoid extra memory usage
merged_df = merged_df.groupby('instrument_token', group_keys=False).apply(calculate_indicators, periods)

# Resetting the index after processing
merged_df = merged_df.reset_index(drop=False)

# Display the last few rows to verify the calculations
columns_to_display = ['timestamps', 'instrument_token', 'close_price',
                      'CCI_9', 'CCI_14', 'CCI_21']
print(merged_df[columns_to_display].tail())

# check nan values in data frame
print_nan_counts(merged_df)
# Using the function to fill NaN values in merged_df
merged_df_filled = fill_nan_values(merged_df)
# check nan values in data frame
print_nan_counts(merged_df)

# diff_52_week_high
# diff_from_52_week_low in % terms

# Ensure the 'timestamps' column is in datetime format
merged_df['timestamps'] = pd.to_datetime(merged_df['timestamps'])

# If the DataFrame has a multi-level index, handle it properly
if 'instrument_token' in merged_df.index.names or 'level_1' in merged_df.index.names:
    # Rename the index levels to avoid conflicts when they are reset to columns
    merged_df = merged_df.rename_axis(index={name: f'index_{name}' for name in merged_df.index.names})

# Now safely reset the index
merged_df = merged_df.reset_index()

# Ensure no duplicate columns after reset
merged_df = merged_df.loc[:, ~merged_df.columns.duplicated()]

# Sort the DataFrame by 'instrument_token' and 'timestamps'
merged_df.sort_values(by=['instrument_token', 'timestamps'], inplace=True)

# Define a small number to prevent division by zero
epsilon = 1e-8

# Calculate the percentage difference from the 52-week high and low for each 'instrument_token'
merged_df['%diff_52_week_high'] = merged_df.groupby('instrument_token')['close_price'].transform(
    lambda x: (x - x.rolling(window=252).max()) / (x.rolling(window=252).max() + epsilon) * 100
)
merged_df['%diff_from_52_week_low'] = merged_df.groupby('instrument_token')['close_price'].transform(
    lambda x: (x - x.rolling(window=252).min()) / (x.rolling(window=252).min() + epsilon) * 100
)

# Display the last few rows to confirm the new columns
print(merged_df[['timestamps', 'instrument_token', 'close_price', '%diff_52_week_high', '%diff_from_52_week_low']].tail())

# check nan values in data frame
print_nan_counts(merged_df)
# Using the function to fill NaN values in merged_df
merged_df_filled = fill_nan_values(merged_df)
# # check nan values in data frame
print_nan_counts(merged_df)

def calculate_pvroc_and_volume_roc(df, period=14):
    # Define a small number to prevent division by zero
    epsilon = 1e-8

    # Calculate the Rate of Change for the price, adding epsilon to prevent division by zero
    price_roc = df['close_price'].pct_change(periods=period).replace([np.inf, -np.inf], np.nan) * 100

    # Calculate the Rate of Change for the volume, adding epsilon to prevent division by zero
    volume_roc = df['volume'].pct_change(periods=period).replace([np.inf, -np.inf], np.nan) * 100

    df[f'Volume_ROC_{period}'] = volume_roc  # Store the volume rate of change
    df[f'PVROC_{period}'] = price_roc * volume_roc  # Combine the two to get a Price-Volume Rate of Change

    return df


# Apply the PVROC and Volume ROC function to the DataFrame
periods = [7, 14, 28, 56, 252]  # You can choose periods that make sense for your analysis
for period in periods:
    merged_df = calculate_pvroc_and_volume_roc(merged_df, period)

# Display the first few rows to confirm the new columns
print(merged_df[['timestamps', 'instrument_token', 'close_price', 'volume', 'Volume_ROC_7', 'PVROC_7', 'Volume_ROC_14', 'PVROC_14', 'Volume_ROC_28', 'PVROC_28', 'Volume_ROC_56', 'PVROC_56', 'Volume_ROC_252', 'PVROC_252']].tail())

def calculate_obv(df):
    # Calculate the daily price change
    price_change = df['close_price'].diff()

    # Determine the direction of trade for each day
    direction = price_change.apply(np.sign)

    # Calculate OBV by cumulatively summing the volume adjusted by its trading direction
    obv = (direction * df['volume']).cumsum()

    return obv

# Apply the OBV function to the DataFrame, grouped by 'instrument_token'
merged_df['OBV'] = merged_df.groupby('instrument_token').apply(calculate_obv).reset_index(level=0, drop=True)

# Display the first few rows to confirm the new column
print(merged_df[['timestamps', 'instrument_token', 'close_price', 'volume', 'OBV']].tail())

# check nan values in data frame
print_nan_counts(merged_df)
# Using the function to fill NaN values in merged_df
merged_df = fill_nan_values(merged_df)
# # check nan values in data frame
print_nan_counts(merged_df)

def calculate_volume_rsi(df, period=14):
    # Calculate daily volume change
    delta_volume = df['volume'].diff()

    # Separate the up-volume and down-volume
    up_volume = delta_volume.clip(lower=0)
    down_volume = -delta_volume.clip(upper=0)

    # Calculate the exponential moving average of up-volume and down-volume
    avg_up_volume = up_volume.rolling(window=period).mean()
    avg_down_volume = down_volume.rolling(window=period).mean()

    # Calculate the Volume RSI
    RS = avg_up_volume / avg_down_volume
    volume_rsi = 100 - (100 / (1 + RS))

    return volume_rsi

# Apply the Volume RSI function to each group in the DataFrame
merged_df['Volume_RSI_14'] = merged_df.groupby('instrument_token').apply(lambda x: calculate_volume_rsi(x, 14)).reset_index(level=0, drop=True)

# Display the first few rows to confirm the new column
print(merged_df[['timestamps', 'instrument_token', 'close_price', 'volume', 'Volume_RSI_14']].tail())

# check nan values in data frame
print_nan_counts(merged_df)
# Using the function to fill NaN values in merged_df
merged_df = fill_nan_values(merged_df)
# # check nan values in data frame
print_nan_counts(merged_df)

def calculate_vpt(df):
    # Calculate percentage change in price
    price_change = df['close_price'].pct_change()

    # Calculate the VPT
    vpt = (df['volume'] * price_change).cumsum()

    return vpt

# Apply the VPT function to each group in the DataFrame
merged_df['VPT'] = merged_df.groupby('instrument_token').apply(lambda x: calculate_vpt(x)).reset_index(level=0, drop=True)

# Display the first few rows to confirm the new column
print(merged_df[['timestamps', 'instrument_token', 'close_price', 'volume', 'VPT']].tail())

def calculate_mfi(group, period=14):
    # Calculate Typical Price
    typical_price = (group['high_price'] + group['low_price'] + group['close_price']) / 3

    # Calculate Raw Money Flow
    raw_money_flow = typical_price * group['volume']

    # Shift the typical price to compare with the previous day
    shifted_typical_price = typical_price.shift(1)

    # Calculate Positive and Negative Money Flow
    positive_flow = pd.Series(np.where(typical_price > shifted_typical_price, raw_money_flow, 0), index=group.index)
    negative_flow = pd.Series(np.where(typical_price < shifted_typical_price, raw_money_flow, 0), index=group.index)

    # Calculate the 14-period sum of Positive and Negative Money Flow
    positive_flow_sum = positive_flow.rolling(window=period).sum()
    negative_flow_sum = negative_flow.rolling(window=period).sum()

    # Calculate Money Flow Ratio
    money_flow_ratio = positive_flow_sum / negative_flow_sum

    # Calculate Money Flow Index
    mfi = 100 - (100 / (1 + money_flow_ratio))

    return mfi

# Apply the MFI function to each group in the DataFrame and compute MFI
merged_df['MFI_14'] = merged_df.groupby('instrument_token', group_keys=False).apply(calculate_mfi)

# Display the first few rows to confirm the new column
print(merged_df[['timestamps', 'instrument_token', 'close_price', 'volume', 'MFI_14']].tail())

def calculate_cmo(df, period=10):
    # Calculate price change from previous day
    delta = df['close_price'].diff()

    # Sum of gains and losses
    gain = delta.where(delta > 0, 0).rolling(window=period).sum()
    loss = -delta.where(delta < 0, 0).rolling(window=period).sum()

    # Calculate the Chande Momentum Oscillator
    cmo = ((gain - loss) / (gain + loss)) * 100

    return cmo

# Apply the CMO function to each group in the DataFrame
merged_df['CMO_10'] = merged_df.groupby('instrument_token').apply(lambda x: calculate_cmo(x)).reset_index(level=0, drop=True)

# Display the first few rows to confirm the new column
print(merged_df[['timestamps', 'instrument_token', 'close_price', 'CMO_10']].tail())

# check nan values in data frame
print_nan_counts(merged_df)
# Using the function to fill NaN values in merged_df
merged_df = fill_nan_values(merged_df)
# # check nan values in data frame
print_nan_counts(merged_df)

print(merged_df.columns)

print(len(merged_df))

total_memory_usage = merged_df.memory_usage(deep=True).sum()
print(f"Total memory usage: {total_memory_usage / (1024**2)} MB")

desired_chunk_size_mb = 500  # Desired chunk size in MB
row_memory = total_memory_usage / len(merged_df)  # Average memory usage per row
chunk_size = int((desired_chunk_size_mb * (1024**2)) / row_memory)  # Number of rows per chunk
print(f"Chunk size: {chunk_size} rows")

# # saving merged_df in drive
# merged_df.to_csv('/content/drive/My Drive/merged_data.csv', index=True)  # Save to Drive

# chunk_size = 100000  # Adjust based on your needs
# for i in range(0, len(merged_df), chunk_size):
#     chunk = merged_df.iloc[i:i+chunk_size]
#     chunk.to_csv(f'/content/drive/My Drive/stocks_data/merged_data_part_{i//chunk_size}.csv', index=True)

# Define the list of indicators to be used for momentum score calculation
# List of all columns for which you want to calculate the correlation
indicators = [
    'EMA_5', 'EMA_75', 'EMA_200',
              'RSI_9', 'RSI_14', 'RSI_21', 'mean_deviation_9', 'mean_deviation_14', 'mean_deviation_21',
    'CCI_9', 'CCI_14', 'CCI_21', 'MACD', 'MACD_signal',
           '%change_close_VWAP_monthly', '%change_close_VWAP_yearly', '%change_close_VWAP',
              '%diff_52_week_high', '%diff_from_52_week_low',
    'Volume_ROC_7', 'PVROC_7', 'Volume_ROC_14', 'PVROC_14', 'Volume_ROC_28', 'PVROC_28',
    'Volume_ROC_56', 'PVROC_56', 'Volume_ROC_252', 'PVROC_252', 'OBV', 'Volume_RSI_14',
    'CMO_10', 'volume',
    'INDIA VIX', 'NIFTY 50', 'NIFTY 500',
       'NIFTY AUTO', 'NIFTY BANK',
       'NIFTY COMMODITIES', 'NIFTY CONSR DURBL', 'NIFTY CONSUMPTION',
       'NIFTY CPSE', 'NIFTY ENERGY',
       'NIFTY FMCG',
       'NIFTY HEALTHCARE', 'NIFTY INDIA MFG',
        'NIFTY IT',
       'NIFTY MEDIA', 'NIFTY METAL', 'NIFTY MICROCAP250',
       'NIFTY PHARMA', 'NIFTY PSE', 'NIFTY PSU BANK', 'NIFTY PVT BANK',
       'NIFTY REALTY',  'NIFTY SMLCAP 100']

# Convert indicator columns to numeric, handling errors and ensuring no data loss
for column in indicators:
    merged_df[column] = pd.to_numeric(merged_df[column], errors='coerce')

# Function to normalize indicators and calculate momentum score
def normalize_and_calculate_momentum(group):
    normalized = (group[indicators] - group[indicators].min()) / (group[indicators].max() - group[indicators].min())
    return normalized.mean(axis=1)

# Apply the function and calculate the momentum score for each instrument_token
momentum_scores = merged_df.groupby('instrument_token').apply(normalize_and_calculate_momentum)

# Ensure that the resulting series aligns with the DataFrame's index
merged_df['momentum_score'] = momentum_scores.values  # Assign the values directly to match the DataFrame's structure

# Display the last few rows to confirm the new column
print(merged_df[['timestamps', 'instrument_token', 'close_price', 'momentum_score']].tail())

# prompt: print all the columns of merged_df, all columns, print(merged_df.columns) wont print all columns

# Print all columns using a loop
for column in merged_df:
    print(column)

# prompt: free unused system RAM to optimise the performance

import gc
gc.collect()

"""Data Analysis"""

# prompt: Perform statistical analysis on the various technical indicators to understand their relationships and identify potential trading opportunities.

# Perform statistical analysis on the technical indicators

import seaborn as sns
import matplotlib.pyplot as plt

# List of all columns for which you want to calculate the correlation
columns = ['open_price', 'high_price', 'low_price', 'close_price',
    'EMA_5', 'EMA_75', 'EMA_200',
              'RSI_9', 'RSI_14', 'RSI_21', 'mean_deviation_9', 'mean_deviation_14', 'mean_deviation_21',
    'CCI_9', 'CCI_14', 'CCI_21', 'MACD', 'MACD_signal',

           '%change_close_VWAP_monthly', '%change_close_VWAP_yearly', '%change_close_VWAP',
              '%diff_52_week_high', '%diff_from_52_week_low',
    'Volume_ROC_7', 'PVROC_7', 'Volume_ROC_14', 'PVROC_14', 'Volume_ROC_28', 'PVROC_28',
    'Volume_ROC_56', 'PVROC_56', 'Volume_ROC_252', 'PVROC_252', 'OBV', 'Volume_RSI_14',
    'CMO_10', 'volume',
    'INDIA VIX', 'NIFTY 50', 'NIFTY 500',
       'NIFTY AUTO', 'NIFTY BANK',
       'NIFTY COMMODITIES', 'NIFTY CONSR DURBL', 'NIFTY CONSUMPTION',
       'NIFTY CPSE', 'NIFTY ENERGY',
       'NIFTY FMCG',
       'NIFTY HEALTHCARE', 'NIFTY INDIA MFG',
        'NIFTY IT',
       'NIFTY MEDIA', 'NIFTY METAL', 'NIFTY MICROCAP250',
       'NIFTY PHARMA', 'NIFTY PSE', 'NIFTY PSU BANK', 'NIFTY PVT BANK',
       'NIFTY REALTY',  'NIFTY SMLCAP 100']

# Calculate correlations among the selected columns
correlations = merged_df[columns].corr()

# Plotting the correlation matrix
plt.figure(figsize=(24, 22))  # Adjust size to better fit the increased number of variables
mask = np.triu(np.ones_like(correlations, dtype=bool))
sns.heatmap(correlations, mask=mask, cmap='coolwarm', square=True, linewidths=.5, cbar_kws={"shrink": .5})
plt.xticks(rotation=45, fontsize=8)  # Adjust font size as needed
plt.yticks(fontsize=8)
plt.title("Correlation Matrix of Stock Data")
plt.show()



# Analyze the correlations to identify potential trading opportunities
# For example, if two indicators are highly correlated, you might consider using one as a confirmation for the other when making trading decisions.

# Perform other statistical analysis as needed, such as calculating standard deviations, skewness, and kurtosis.

# prompt: count number of columns in merged_df

print(len(merged_df.columns))

# Find the range of momentum_score across the entire DataFrame
momentum_range = {
    'min_momentum': merged_df['momentum_score'].min(),
    'max_momentum': merged_df['momentum_score'].max()
}

print(momentum_range)

# Assuming merged_df is already defined and contains the columns 'instrument_token', 'timestamps', and 'close_price'

# Sort the DataFrame
merged_df.sort_values(by=['instrument_token', 'timestamps'], inplace=True)

# Define a small number to prevent division by zero
epsilon = 1e-8

# Calculate the return over the next 22 days and 10 days
# Shift the close price 22 days back and 10 days back and then calculate the return
merged_df['future_return_10'] = (merged_df.groupby('instrument_token')['close_price'].shift(-10) / (merged_df['close_price'] + epsilon)) - 1
merged_df['future_return_22'] = (merged_df.groupby('instrument_token')['close_price'].shift(-22) / (merged_df['close_price'] + epsilon)) - 1

# Create a column where 1 indicates a return greater than 10% in the next 22 days, and 0 otherwise
merged_df['above_10_percent_return_next_10_days'] = (merged_df['future_return_10'] > 0.10).astype(int)
merged_df['above_10_percent_return_next_22_days'] = (merged_df['future_return_22'] > 0.10).astype(int)

# Showing the modified DataFrame
print(merged_df[['instrument_token', 'timestamps', 'close_price', 'above_10_percent_return_next_10_days', 'above_10_percent_return_next_22_days']].tail())

# Check for NaN values in each column and get their count
nan_counts = merged_df.isna().sum()

# Filter columns that have NaN values and their counts
nan_columns = nan_counts[nan_counts > 0]

# Print each column name and its NaN count explicitly
for column, count in nan_columns.items():
    print(f'{column}: {count}')

# Iterate over each column and round only if it's numeric
# Including integers, floats, and any other numeric types
for column in merged_df.select_dtypes(include=['number']):
    merged_df[column] = merged_df[column].astype('float').round(2)

# Show the result
print(merged_df)

# Import necessary libraries
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import SGDClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.impute import SimpleImputer  # Import SimpleImputer
import numpy as np

# Assuming 'merged_df' has the necessary features and target variable
X = merged_df.drop(['above_10_percent_return_next_10_days', 'tradingsymbol'], axis=1)
y = merged_df['above_10_percent_return_next_10_days']

# Convert datetime columns to Unix timestamp (if any)
datetime_cols = X.select_dtypes(include=['datetime64']).columns
for col in datetime_cols:
    X[col] = X[col].astype('int64') // 10**9

# Impute missing values
imputer = SimpleImputer(strategy='mean')  # You can choose 'median' or 'most_frequent' as well
X = imputer.fit_transform(X)

# Check and handle infinite values
X = np.where(np.isfinite(X), X, np.nan)  # Replace infinities with NaN
imputer = SimpleImputer(strategy='mean')  # You can re-use or create a new imputer
X = imputer.fit_transform(X)  # Impute any NaNs created

# Split dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# SGDClassifier for Incremental Learning
clf = SGDClassifier(random_state=42)

# Incremental learning
batch_size = 1000  # Adjust based on your memory capacity and dataset size
for start in range(0, X_train.shape[0], batch_size):
    end = min(start + batch_size, X_train.shape[0])
    clf.partial_fit(X_train[start:end], y_train[start:end], classes=np.unique(y_train))

# Make predictions and evaluate the model
y_pred_sgd = clf.predict(X_test)
accuracy_sgd = accuracy_score(y_test, y_pred_sgd)
report_sgd = classification_report(y_test, y_pred_sgd)

# Print model evaluation results for SGDClassifier
print(f'SGD Classifier Accuracy: {accuracy_sgd}\nSGD Classifier Classification Report:\n{report_sgd}')

import joblib

# Save the model
joblib.dump(clf, 'sgd_classifier_model.joblib')

# Assuming you have 'close_price' and 'timestamps' in your merged_df

# Get initial and final price for the absolute return calculation
initial_price = merged_df['close_price'].iloc[0]
final_price = merged_df['close_price'].iloc[-1]

# Calculate absolute return
absolute_return = (final_price - initial_price) / initial_price

# Calculate the number of years for the period
period_years = (merged_df['timestamps'].iloc[-1] - merged_df['timestamps'].iloc[0]).days / 365.25

# Calculate CAGR
cagr = ((final_price / initial_price) ** (1 / period_years)) - 1

# Backtesting - simulating trades based on the model's predictions
# Assume that we invest in the instrument when the model predicts a return greater than 10% in next 10 or 22 days
# This is a simplified example and may need adjustment for actual trading strategy considerations
merged_df['predicted'] = clf.predict(X)  # Full dataset predictions
merged_df['strategy_return'] = merged_df['predicted'] * merged_df['future_return_10']  # Replace 'future_return_10' as needed

# Total strategy return is the sum of individual predicted period returns
total_strategy_return = merged_df['strategy_return'].sum()

# Display the results
print(f'Absolute Return: {absolute_return * 100:.2f}%')
print(f'CAGR: {cagr * 100:.2f}%')
print(f'Total Strategy Return: {total_strategy_return * 100:.2f}%')

# Create a DataFrame to hold trade details
trade_details = []

# Iterate through the DataFrame to simulate trades
for i in range(len(merged_df) - 10):  # Assuming 10-day future return; adjust as necessary
    if merged_df['predicted'].iloc[i] == 1:  # If the model predicts a positive return
        entry_price = merged_df['close_price'].iloc[i]
        exit_price = merged_df['close_price'].iloc[i + 10]  # Assuming we hold for 10 days

        # Avoid division by zero by ensuring entry_price is not zero
        if entry_price == 0:
            continue  # Skip this trade or handle it in a manner suitable for your context

        trade_return = (exit_price - entry_price) / entry_price
        trade_details.append({
            'entry_date': merged_df['timestamps'].iloc[i],
            'exit_date': merged_df['timestamps'].iloc[i + 10],
            'entry_price': entry_price,
            'exit_price': exit_price,
            'trade_return': trade_return
        })

# Convert trade details into a DataFrame for easier analysis
trade_details_df = pd.DataFrame(trade_details)

# Now you can analyze trade_details_df for individual trade performances
print(trade_details_df.tail())  # Show the last few trades

# Update total strategy return to use trade details instead of cumulative sum
total_strategy_return = trade_details_df['trade_return'].sum()

# Display the updated results
print(f'Absolute Return: {absolute_return * 100:.2f}%')
print(f'CAGR: {cagr * 100:.2f}%')
print(f'Total Strategy Return: {total_strategy_return * 100:.2f}%')

# Export the DataFrame to a CSV file
trade_details_df.to_csv('trade_details.csv', index=False)

# Use the Colab 'files' module to download the file to your local machine
from google.colab import files
files.download('trade_details.csv')

from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Dense
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import StandardScaler
import pandas as pd

# Assuming merged_df is preloaded and has the necessary features including a 'timestamps' column

# Convert timestamps to Unix time (numeric)
merged_df['timestamps'] = pd.to_datetime(merged_df['timestamps']).astype('int64') // 10**9

# Define features (X) and target (y)
X = merged_df.drop(['above_10_percent_return_next_10_days', 'tradingsymbol'], axis=1)
y = merged_df['above_10_percent_return_next_10_days']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Free memory
del X, y, merged_df

# Normalize the feature data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Free memory
del X_train, X_test

# Initialize the neural network model
model = Sequential([
    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),
    Dense(64, activation='relu'),
    Dense(1, activation='sigmoid')
])

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X_train_scaled, y_train, epochs=20, batch_size=32)

# Save the model
model.save('my_model.h5')

# Free memory
del X_train_scaled, y_train

# Predict on the testing set
y_pred = model.predict(X_test_scaled)
y_pred = (y_pred > 0.5).astype(int)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

# Print results
print(f'Accuracy: {accuracy}')
print(f'Classification Report:\n{report}')

# Free memory if needed
del model, X_test_scaled, y_test, y_pred

# To load the model later, you can use:
# model = load_model('my_model.h5')